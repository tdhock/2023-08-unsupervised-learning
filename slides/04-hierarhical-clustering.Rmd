---
title: "Hierarchical Clustering"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

# Visualize iris data with labels

```{r}
library(ggplot2)
color.code <- c(
  setosa="#1B9E77",
  versicolor="#D95F02",
  virginica="#7570B3",
  "1"="#E7298A",
  "2"="#66A61E",
  "3"="#E6AB02", 
  "4"="#A6761D")
library(data.table)
iris.dt <- data.table(iris)[(1:150-1) %% 50 < 5]
gg <- ggplot()+
  scale_color_manual(values=color.code)+
  geom_point(aes(
    Petal.Length, Petal.Width, color=Species),
    data=iris.dt)+
  coord_equal()
directlabels::direct.label(gg, "smart.grid")
```

---

# Visualize iris data without labels

- Let $X = [x_1 \cdots x_n]^\intercal \in\mathbb R^{n\times p}$ be the
  data matrix (input for clustering), where $x_i\in\mathbb R^p$ is the
  input vector for observation $i$.
- Example iris $n=150$ observations, $p=2$ dimensions.

```{r results=TRUE}
data.mat <- as.matrix(
  iris.dt[, c("Petal.Width","Petal.Length")])
data.mat[1:4,]
```

```{r fig.height=3}
theme_set(
  ##theme_bw()+theme(panel.spacing=grid::unit(0, "lines"))
  theme_grey()+theme(text=element_text(size=20))
)
ggplot()+
  geom_point(aes(
    Petal.Length, Petal.Width),
    data=iris.dt)+
  coord_equal()
```

---

# Which pair of rows is most similar?

This is a visualization of 15 rows and two columns from the iris data.

```{r}
data.mat.dt <- data.table(
  row=as.integer(row(data.mat)),
  col=as.integer(col(data.mat)),
  cm=as.numeric(data.mat))
revfac <- function(x){
  factor(x, sort(unique(x), decreasing = TRUE))
}
ggplot()+
  geom_tile(aes(
    factor(col), revfac(row), fill=cm),
    data=data.mat.dt)+
  geom_text(aes(
    factor(col), revfac(row), label=cm),
    data=data.mat.dt)+
  scale_y_discrete("row")+
  scale_x_discrete("column")+
  scale_fill_gradient(low="white", high="red")
```

---

# Hyper-parameter choices (must be fixed prior to learning)

How to compute similarity/distance between rows? 

- Let $x,x'\in\mathbb R^p$ be two feature vectors (rows of data
  matrix).
- L1/manhattan distance: $||x-x'||_1 = \sum_{j=1}^p |x_j-x'_j|$.
- L2/euclidean distance: $||x-x'||_2 = \sqrt{\sum_{j=1}^p (x_j-x'_j)^2}$.

How to compute distance with a group/cluster?
There are several rules, or agglomeration methods:

- single: min distance from any point,
- complete: max distance from any point,
- average: mean distance over all points,
- there are others.


---

# Hierarchical clustering inputs a pairwise distance matrix

```{r}
library(data.table)
iris.d <- dist(data.mat)
iris.d.mat <- as.matrix(iris.d)
iris.d.dt <- data.table(
  row=as.integer(row(iris.d.mat)),
  col=as.integer(col(iris.d.mat)),
  dist=as.numeric(iris.d.mat))
ggplot()+
  geom_tile(aes(
    factor(col), revfac(row), fill=dist),
    data=iris.d.dt)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal(expand=FALSE)+
  scale_y_discrete("observation")+
  scale_x_discrete("observation")
```

---

# Only need lower triangle (symmetry)

```{r}
lower.triangle <- iris.d.dt[col<row]
ggplot()+
  geom_tile(aes(
    factor(col), revfac(row), fill=dist),
    data=lower.triangle)+
  geom_text(aes(
    factor(col), revfac(row), label=sprintf("%.2f", dist)),
    data=lower.triangle)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal(expand=FALSE)+
  scale_y_discrete("observation", limits=paste(nrow(data.mat):1))+
  scale_x_discrete("observation", limits=factor(1:nrow(data.mat)))
```

---

# Find the closest pairs

```{r}
lower.triangle[, min.dist := min(dist)==dist]
ggplot()+
  geom_tile(aes(
    factor(col), revfac(row), fill=dist, color=min.dist),
    size=0.5,
    data=lower.triangle)+
  geom_text(aes(
    factor(col), revfac(row), label=sprintf("%.2f", dist)),
    data=lower.triangle)+
  scale_fill_gradient(low="white", high="red")+
  scale_color_manual(
    values=c("TRUE"="black", "FALSE"="transparent"),
    guide=guide_legend(override.aes = list(fill=NA)))+
  coord_equal(expand=FALSE)+
  scale_y_discrete("observation", limits=paste(nrow(data.mat):1))+
  scale_x_discrete("observation", limits=factor(1:nrow(data.mat)))
```

---

# Join one of the closest pairs (iteration 1)

```{r}
lower.triangle[, to.join := .I %in% which(min.dist)[1] ]
color.values <- c("TRUE"="black", "FALSE"="transparent")
gg.dist <- function(){
  ggplot()+
    ggtitle(paste0("min dist=", lower.triangle[min.dist==TRUE]$dist[1]))+
    geom_tile(aes(
      factor(col), revfac(row), fill=dist,
      color=factor(to.join, names(color.values))),
      size=0.5,
      data=lower.triangle)+
    geom_text(aes(
      factor(col), revfac(row), label=sprintf("%.2f", dist)),
      data=lower.triangle)+
    scale_fill_gradient(low="white", high="red", limits=range(iris.d.dt$dist))+
    scale_color_manual(
      "To join",
      values=color.values,
      breaks=names(color.values),
      drop=FALSE,
      guide=guide_legend(override.aes = list(fill=NA)))+
    coord_equal(expand=TRUE)+
    scale_y_discrete("cluster", limits=paste(nrow(data.mat):1))+
    scale_x_discrete("cluster", limits=factor(1:nrow(data.mat)))
}
gg.dist()
```

---

# iteration 2

```{r}
do.join <- function(){
  join.indices <- lower.triangle[to.join==TRUE, c(row,col)]
  to.remove <- lower.triangle[, row %in% join.indices | col %in% join.indices]
  old.dt <- lower.triangle[to.remove]
  old.dt[, other := ifelse(
    row %in% join.indices, ifelse(
      col %in% join.indices, NA, col), row)]
  new.dt <- old.dt[!is.na(other), .(
    dist=min(dist), N=.N
  ), by=other]
  new.dt[, keep := join.indices[1] ]
  lower.triangle <<- rbind(
    lower.triangle[!to.remove, data.table(row, col, dist)],
    new.dt[, data.table(
      row=ifelse(keep<other, other, keep),
      col=ifelse(keep<other, keep, other),
      dist)])
  lower.triangle[, min.dist := min(dist)==dist]
  lower.triangle[, to.join := .I %in% which(min.dist)[1] ]
  gg.dist()
}
do.join()
```

---

# iteration 3

```{r}
do.join()
```

---

# iteration 4

```{r}
do.join()
```

---

# iteration 5

```{r}
do.join()
```

---

# iteration 6

```{r}
do.join()
```

---

# iteration 7

```{r}
do.join()
```

---

# iteration 8

```{r}
do.join()
```

---

# iteration 9

```{r}
do.join()
```

---

# iteration 10

```{r}
do.join()
```

---

# iteration 11

```{r}
do.join()
```

---

# iteration 12

```{r}
do.join()
```

---

# iteration 13

```{r}
do.join()
```

---

# iteration 14

```{r}
do.join()
```

---

# Visualization of dendrogram (tree diagram)

```{r}
hc.tree <- hclust(iris.d, method="single")
hc.tree.list <- ggdendro::dendro_data(hc.tree)
labels.dt <- data.table(hc.tree.list$labels)
labels.dt[, Species := iris.dt$Species[as.integer(label)] ]
ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    data=hc.tree.list[["segments"]])+
  scale_color_manual(values=color.code[paste(unique(labels.dt$Species))])+
  geom_point(aes(
    x, y, color=Species),
    data=labels.dt)+
  geom_text(aes(
    x, y, label=label),
    vjust=1.1,
    data=labels.dt)+
  scale_y_continuous(
    "distance")+
  scale_x_continuous(
    "observation",
    breaks=NULL)
```

---

# Cutting the tree to get two clusters

```{r}
cut.dist <- 1
cluster.vec <- cutree(hc.tree, h=cut.dist)
labels.dt[, cluster := factor(cluster.vec[as.integer(label)]) ]
ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    data=hc.tree.list[["segments"]])+
  scale_color_manual(values=color.code[paste(unique(labels.dt$cluster))])+
  geom_hline(yintercept=cut.dist, size=1)+
  geom_point(aes(
    x, y, color=cluster),
    data=labels.dt)+
  geom_text(aes(
    x, y, label=label),
    vjust=1.1,
    data=labels.dt)+
  scale_y_continuous(
    "distance")+
  scale_x_continuous(
    "observation",
    breaks=NULL)
```

---

# Cutting the tree to get three clusters

```{r}
cut.dist <- 0.53
cluster.vec <- cutree(hc.tree, h=cut.dist)
labels.dt[, cluster := factor(cluster.vec[as.integer(label)]) ]
ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    data=hc.tree.list[["segments"]])+
  scale_color_manual(values=color.code[paste(unique(labels.dt$cluster))])+
  geom_hline(yintercept=cut.dist, size=1)+
  geom_point(aes(
    x, y, color=cluster),
    data=labels.dt)+
  geom_text(aes(
    x, y, label=label),
    vjust=1.1,
    data=labels.dt)+
  scale_y_continuous(
    "distance")+
  scale_x_continuous(
    "observation",
    breaks=NULL)
```

---

# Cutting the tree to get four clusters

```{r}
cut.dist <- 0.5
cluster.vec <- cutree(hc.tree, h=cut.dist)
labels.dt[, cluster := factor(cluster.vec[as.integer(label)]) ]
ggplot()+
  geom_segment(aes(
    x, y, xend=xend, yend=yend),
    data=hc.tree.list[["segments"]])+
  scale_color_manual(values=color.code[paste(unique(labels.dt$cluster))])+
  geom_hline(yintercept=cut.dist, size=1)+
  geom_point(aes(
    x, y, color=cluster),
    data=labels.dt)+
  geom_text(aes(
    x, y, label=label),
    vjust=1.1,
    data=labels.dt)+
  scale_y_continuous(
    "distance")+
  scale_x_continuous(
    "observation",
    breaks=NULL)
```

---

# ARI computation 

```{r}
cluster.mat <- cutree(hc.tree, 1:nrow(data.mat))
ari.dt.list <- list()
for(n.clusters in 1:ncol(cluster.mat)){
  ARI <- pdfCluster::adj.rand.index(
    cluster.mat[, n.clusters], iris.dt$Species)
  ari.dt.list[[n.clusters]] <- data.table(
    n.clusters,
    ARI)
}
ari.dt <- do.call(rbind, ari.dt.list)

ggplot()+
  geom_point(aes(
    n.clusters, ARI),
    data=ari.dt)
```

---

# distance plot

```{r}
N <- nrow(data.mat)
after.dist <- data.table(
  join.dist=hc.tree$height,
  before=N:2,
  clusters=seq(N-1, 1))
ggplot()+
  geom_point(aes(
    clusters, join.dist),
    data=after.dist)
```

---

# Another 2d data set

```{r}
data(spirals, package="kernlab")
spirals.dt <- data.table(spirals)
ggplot()+
  geom_point(aes(
    V1, V2),
    data=spirals.dt)+
  coord_equal()
```

---

# K-means fails

```{r}
kmeans.result <- kmeans(spirals, 2)
spirals.dt[, cluster := factor(kmeans.result$cluster) ]
ggplot()+
  geom_point(aes(
    V1, V2, color=cluster),
    data=spirals.dt)+
  coord_equal()
```

---

# Hierarchical with two clusters fails

```{r}
spirals.dist <- dist(spirals)
linkage.vec <- c(
  "single", "average", "complete", "median",
  "centroid", "ward.D", "ward.D2", "mcquitty")
spirals.tree.list <- list()
N <- nrow(spirals)
spirals.dplot.list <- list()
for(linkage in linkage.vec){
  hc.tree <- hclust(spirals.dist, linkage)
  spirals.tree.list[[linkage]] <- hc.tree
  spirals.dplot.list[[linkage]] <- data.table(
    linkage,
    join.dist=hc.tree$height,
    clusters=seq(N-1, 1))
}
spirals.dplot <- do.call(rbind, spirals.dplot.list)
spirals.plot <- function(n.clusters){
  spirals.hc.list <- list()
  for(linkage in linkage.vec){
    hc.tree <- spirals.tree.list[[linkage]]
    spirals.dt[, cluster := factor(cutree(hc.tree, k=n.clusters)) ]
    spirals.hc.list[[linkage]] <- data.table(
      linkage, spirals.dt)
  }
  spirals.hc <- do.call(rbind, spirals.hc.list)
  ggplot()+
    geom_point(aes(
      V1, V2, color=cluster),
      data=spirals.hc)+
    coord_equal()+
    facet_wrap("linkage", labeller=label_both, nrow=2)
}
spirals.plot(2)
```

---

# Hierarchical with single linkage and 3 clusters better

```{r}
spirals.plot(3)
```

---

# Distance plots

```{r}
ggplot()+
  geom_point(aes(
    clusters, join.dist),
    data=spirals.dplot[clusters<=20])+
  facet_wrap("linkage", labeller=label_both, nrow=2, scales="free")
```

---

# Gene expression clustering (cancer classes)

Small Round Blue Cell Tumors (SRBCT) of childhood cancer study of Khan
et al. (2001).

```{r}
## https://rdrr.io/cran/plsgenomics/man/SRBCT.html
data(SRBCT, package='plsgenomics')
code.dt <- nc::capture_all_str(
  "29 cases of Ewing sarcoma (EWS), coded 1, 11 cases of Burkitt lymphoma (BL), coded 2, 18 cases of neuroblastoma (NB), coded 3, 25 cases of rhabdomyosarcoma (RMS), coded 4. A total of 63 training samples and 25 test samples are provided in Khan et al. (2001). Five of the test set are non-SRBCT and are not considered here.",
  "cases of ",
  name=".*?",
  " [(]",
  abbrev=".*?",
  "[)], coded ",
  code="[1-4]")
SRBCT.dt <- with(SRBCT, data.table(
  sample=as.integer(row(X)),
  gene=as.integer(col(X)),
  cancer.class=code.dt[, factor(Y, code, name)],
  expression=as.numeric(X)
))

ggplot()+
  geom_raster(aes(
    gene, sample, fill=log10(expression)),
    data=SRBCT.dt)+
  scale_fill_gradient(low="white", high="red")
```

---

# Clustering samples

```{r}
data.list <- with(SRBCT, list(
  sample=X,
  gene=t(X)
))
dist.list <- lapply(data.list, dist)
hc.list <- lapply(dist.list, hclust, "average")
ggd.list <- lapply(hc.list, ggdendro::dendro_data)

dim.seg.dt.list <- list()
for(dim.name in names(data.list)){
  ggd <- ggd.list[[dim.name]]
  label.dt <- data.table(ggd[["labels"]])
  dim.seg.dt <- data.table(ggd[["segments"]])
  other.name <- names(data.list)[names(data.list) != dim.name]
  other.max <- nrow(ggd.list[[other.name]][["labels"]])
  max.dist <- dim.seg.dt[, max(c(y,yend))]
  norm <- function(y)0.1*other.max*y/max.dist
  dim.seg.dt[, norm.y := norm(y) ]
  dim.seg.dt[, norm.yend := norm(yend) ]
  orig.dt <- data.table(label=paste(SRBCT.dt[[dim.name]]))
  set(
    SRBCT.dt,
    j=paste0(dim.name, ".ord"),
    value=label.dt[orig.dt, x, on="label"])
  dim.seg.dt.list[[dim.name]] <- dim.seg.dt
}

ggplot()+
  geom_raster(aes(
    gene.ord, sample.ord, fill=log10(expression)),
    data=SRBCT.dt)+
  scale_fill_gradient(low="white", high="red")+
  geom_segment(aes(
    x, -norm.y,
    xend=xend, yend=-norm.yend),
    data=dim.seg.dt.list[["gene"]])+
  geom_segment(aes(
    y=x, x=-norm.y,
    yend=xend, xend=-norm.yend),
    data=dim.seg.dt.list[["sample"]])
    
```

# Possible Exam Questions

What is the big O notation asymptotic time complexity of the following
algorithms in terms of N (number of data observations/rows), P (number
of data features/columns), and K (number of clusters).

- K-means.
- Gaussian mixture model with diagonal covariance matrix.
- Gaussian mixture model with unconstrained covariance matrix.
- Hierarchical clustering with single linkage.

---

# Possible Exam Questions 2

- What are the two hyper-parameters that must be chosen before running
  the hierarchical clustering algorithm?
- For a data set with N=200 observations/rows, how large is the
  pairwise distance matrix? How many iterations of the cluster joining
  occur?
