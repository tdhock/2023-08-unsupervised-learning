---
title: "Binary segmentation"
author: "Toby Dylan Hocking"
output:
  beamer_presentation:
    includes:
      in_header: 05-preamble.tex
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
library(ggplot2)
my.theme <- theme_grey()+
  theme(text=element_text(size=14))
theme_set(my.theme)
```

# Introduction

## Motivation for change-point detection in time series data

- Detecting changes/abnormalities important in medicine.

![Electrocardiograms (heart monitoring), Fotoohinasab et al, Asilomar conference 2020.](intro-ecg)

## Motivation for change-point detection in time series data

- Detecting the time when a spike occurs is important in neuroscience.

![Neural spikes in calcium imaging data, Jewell et al, Biostatistics
2019.](intro-neuroscience)

## Motivation for change-point detection in genomic data sequences

- Detecting breakpoints is important in diagnosis of some types of
  cancer, such as neuroblastoma.

![DNA copy number data, breakpoints associated with aggressive cancer,
Hocking et al, Bioinformatics 2014.](intro-breakpoints)

## Motivation for change-point detection in genomic data sequences

- Detecting peaks (up/down changes) in genomic data is important in
  order to understand which genes are active or inactive.

![ChIP-seq data for characterizing active regions in the human genome, Hocking et al, Bioinformatics 2017.](intro-peaks)

## Segmentation / change-point detection framework

- Let $x_1, \dots, x_n \in\mathbb R$ be a data sequence
  over space or time (logratio column in DNA copy number data below).
- Where are the abrupt changes in the data sequence?

```{r results=TRUE}
data(neuroblastoma, package="neuroblastoma")
library(data.table)
nb.dt <- data.table(neuroblastoma[["profiles"]])
one.dt <- nb.dt[profile.id==4 & chromosome==2]
one.dt
```

## Segmentation / change-point data visualization

- Let $x_1, \dots, x_n \in\mathbb R$ be a data sequence
  over space or time.
- Where are the abrupt changes in the data sequence?

```{r results=TRUE}
ggplot()+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    position/1e6, logratio),
    data=one.dt)
```

## Assume normal distribution with change in mean, constant variance

- There are a certain number of clusters/segments
  $K\in\{1,\dots, n\}$.
- Each segment $k\in\{1,\dots,K\}$ has its own mean
  parameter $\mu_k\in\mathbb R$.
- There is some constant variance parameter $\sigma^2>0$ which is
  common to all segments.
- For each data point $i$ on segment
  $k\in\{1,\dots,K\}$ we have $x_i \sim N(\mu_k, \sigma^2)$ -- normal
  distribution.
- This normal distribution assumption means that we want to find
  segments/change-points with mean $m$ that minimize the square loss,
  $(x-m)^2$.
- Other distributional assumptions / loss functions are possible.

## Visualize data sequence

```{r}
one.dt[, data.i := .I]
gg <- ggplot()+
  scale_x_continuous(
    limits=c(0, nrow(one.dt)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=one.dt)
gg
```

## Simplest model, 1 segment, 0 change-points

```{r}
bs.list <- suppressWarnings(binsegRcpp::binseg_normal(one.dt[["logratio"]]))
bs.models <- bs.list[["splits"]]
model.color <- "blue"
plotK <- function(k){
  k.segs <- coef(bs.list, as.integer(k))
  gg+
    geom_vline(aes(
      xintercept=start-0.5),
      color=model.color,
      data=k.segs[-1])+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      linewidth=2,
      alpha=0.5,
      color=model.color,
      data=k.segs)
}
plotK(1)
```

## Find best single change-point (two segments)

```{r}
plotK(2)
```

## Find next best change-point (given first change)

```{r}
plotK(3)
```

## Find four segments

```{r}
plotK(4)
```

## Find five segments

```{r}
plotK(5)
```

## Find six segments

```{r}
plotK(6)
```

## Find seven segments

```{r}
plotK(7)
```

## Find 50 segments

```{r}
plotK(50)
```

## Find 100 segments

```{r}
plotK(100)
```

## Largest model: `r nrow(one.dt)` segments (changes everywhere)

```{r}
plotK(nrow(one.dt))
```

## Why binary segmentation?

- Dynamic programming computes \alert{one optimal model for a penalty
  $\lambda$ in $O(n \log n)$ time} in sequence of $n$ data (see pruning
  algorithms: PELT---Killick 2012, FPOP---Maidstone 2017, DUST---Truong and Runge 2024).
- Binary segmentation (Scott and Knott 1974) is a simple baseline algorithm.
- In theory, binary segmentation is extremely fast: heuristic sequence of models
  with $1,\dots,K$ segments computed in $O(n \log K)$ time (typical/best case) or $O(n K)$ time (worst case).
- \alert{Complete sequence of $n$ models for $n$ data in only $O(n\log n)$ time!}
- Reviewers of my papers about dynamic programming want to see comparisons with binary segmentation: is there any difference in speed/accuracy?
- If we run existing code for binary segmentation: does it achieve the expected time complexity? Why or why not?

## Error/loss function visualization

- Let $m^{(k)}\in\mathbb R^n$ be the mean vector with $k$ segments.
- Error for $k$ segments is defined as sum of squared difference
  between data $x$ and mean $m^{(k)}$ vectors, $E_k = \sum_{i=1}^n
  (x_i-m^{(k)}_i)^2$
- As in previous clustering models, kink in the error curve can be
  used as a simple model selection criterion.

```{r fig.height=5}
ggplot()+
  geom_line(aes(
    segments, loss),
    data=bs.models)
```

## Error/loss function zoom

```{r}
show.max <- 10
ggplot()+
  geom_point(aes(
    segments, loss),
    data=bs.models[segments<show.max])+
  scale_x_continuous(breaks=1:show.max)
```

## Learning algorithm

- Start with one segment, then repeat:
- Compute loss of each possible split.
- Choose split which results in largest loss decrease.
- If $s = \sum_{i=1}^n x_i$ is the sum over $n$ data points, then the
  mean is $s/n$ and the square loss (from 1 to $n$) is
$$L_{1,n} = \sum_{i=1}^n (x_i - s/n)^2 = \sum_{i=1}^n [x_i^2] - 2(s/n)s + n(s/n)^2 $$
- Given cumulative sums, and a split point $t$, we can compute square
  loss from 1 to $t$, $L_{1,t}$, and from $t+1$ to $n$, $L_{t+1,n}$,
  in constant $O(1)$ time.
- We can minimize over all change-points $t$ in linear $O(n)$ time,
$$\min_{t\in\{1,\dots,n-1\}} L_{1,t} + L_{t+1,n}$$

## First step of binary segmentation

```{r}
x.vec <- one.dt[["logratio"]]
n.data <- length(x.vec)
hjust.vec <- c("before"=1, "after"=0)
before.after <- function(x.or.sq){
  cbind(
    before=cumsum(x.or.sq[-length(x.or.sq)]),
    after=rev(cumsum(rev(x.or.sq[-1]))))
}
end.vec <- seq(1, n.data-1)
n.mat <- cbind(end.vec, rev(end.vec))
const.mat <- before.after(x.vec^2)
s.mat <- before.after(x.vec)
l.mat <- const.mat-s.mat^2/n.mat
err.dt <- data.table(
  loss=l.mat,
  loss.total=rowSums(l.mat),
  mean=s.mat/n.mat,
  end=end.vec)
first.min <- err.dt[which.min(loss.total)]

plotChange <- function(show.i){
  show.change <- err.dt[show.i]
  show.long <- melt(
    show.change,
    measure=c("loss.before", "loss.after"))
  show.segs <- data.table(
    start=c(1, show.i+1),
    end=c(show.i, n.data),
    mean=s.mat[show.i,]/n.mat[show.i,])
  ggplot()+
    geom_point(aes(
      data.i, logratio),
      data=data.table(y="logratio", one.dt))+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      data=show.change)+
    geom_text(aes(
      end+0.5, -Inf, label=sprintf(" total loss=%.4f", loss.total)),
      color=model.color,
      hjust=0,
      vjust=-0.1,
      data=data.table(y="loss", show.change))+
    geom_text(aes(
      end+0.5, -Inf,
      label=sprintf(" %s=%.4f ", variable, value),
      hjust=hjust.vec[sub("loss.","",variable)]),
      color=model.color,
      data=data.table(y="logratio", show.long),
      vjust=-0.1)+
    geom_point(aes(
      end+0.5, loss.total),
      shape=21,
      data=data.table(y="loss", err.dt))+
    facet_grid(y ~ ., scales="free")+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      linewidth=2,
      alpha=0.5,
      color=model.color,
      data=data.table(y="logratio", show.segs))
}
plotChange(31)
```

## First step of binary segmentation

```{r}
plotChange(41)
```

## First step of binary segmentation

```{r}
plotChange(111)
```

## First step of binary segmentation

```{r}
plotChange(157)
```

## Second step of binary segmentation

```{r}
(seg.info <- first.min[, data.table(
  start=c(1, end+1),
  end=c(end, nrow(one.dt)),
  seg=factor(c("before", "after"), c("before", "after")),
  offset=c(0, end))])
seg.ord <- list(first=identity, other=rev)
for(name in names(seg.ord)){
  fun <- seg.ord[[name]]
  for(data.type in c("mean","loss")){
    col.vec <- paste0(data.type,".",c("before","after"))
    orig.ord <- unlist(first.min[, ..col.vec])
    set(seg.info, j=paste0(name, ".", data.type), value=fun(orig.ord))
  }
}
seg.info

one.dt[, i := data.i]
setkey(one.dt, data.i, i)
setkey(seg.info, start, end)
two.dt <- foverlaps(one.dt, seg.info)
##two.dt <- data.table(one.dt)[seg.info, on=.(i <= end, i >= start)]
two.err <- two.dt[, {
  rel.end <- seq(1, .N-1)
  n.mat <- cbind(rel.end, rev(rel.end))
  const.mat <- before.after(logratio^2)
  s.mat <- before.after(logratio)
  l.mat <- const.mat-s.mat^2/n.mat
  data.table(
    mean=s.mat/n.mat,
    loss.this=rowSums(l.mat),
    loss.decrease=first.loss-rowSums(l.mat),
    loss=other.loss+rowSums(l.mat),
    new.end=rel.end+offset)
}, by=names(seg.info)]

plot2 <- function(show.i){
  (show.err <- two.err[show.i])
  show.other <- seg.info[seg != show.err$seg]
  show.change <- show.err[, .(seg, start, end=new.end, mean=mean.before)]
  show.segs <- rbind(
    show.other[, .(seg, start, end, mean=first.mean)],
    show.change,
    show.err[, .(seg, start=new.end+1, end, mean=mean.after)])
  ggplot()+
    geom_point(aes(
      data.i, logratio),
      data=data.table(y="logratio", two.dt))+
    geom_point(aes(
      new.end+0.5, loss),
      shape=21,
      data=data.table(y="loss", two.err))+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      linewidth=2,
      alpha=0.5,
      color=model.color,
      data=data.table(y="logratio", show.segs))+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      data=show.change)+
    facet_grid(y ~ ., scales="free", space="free")+
    scale_x_continuous(
      breaks=seq(0, 1000, by=20))+
    coord_cartesian(expand=TRUE)
}
plot2(15)

```

## Second step of binary segmentation

```{r}
plot2(30)
```

## Second step of binary segmentation

```{r}
plot2(100)
```

## Second step of binary segmentation

```{r}
plot2(112)
```

## Second step of binary segmentation

```{r}
plot2(156)
```

## Second step of binary segmentation

```{r}
plot2(200)
```

## Efficient loss computation

- Minimization can be performed by choosing the split with loss (black
  point) which maximizes the decrease in loss with respect to previous
  model (red, with no split).

```{r}
first.min.long <- nc::capture_melt_single(
  first.min, "loss.",
  seg="before|after", function(x)factor(x, c("before", "after")))
ggplot()+
  geom_hline(aes(
    yintercept=value),
    color="red",
    data=data.table(
      y="loss on this segment", first.min.long))+
  geom_point(aes(
    new.end+0.5, loss),
    shape=21,
    data=data.table(y="total loss", two.err))+
  geom_point(aes(
    new.end+0.5, loss.this),
    shape=21,
    data=data.table(y="loss on this segment", two.err))+
  facet_grid(y ~ seg, scales="free", space="free_x")+
  scale_x_continuous(
    "data.i",
    breaks=seq(0, 1000, by=20))

```

## Learning algorithm, implementation details

First compute the vectors of cumulative sums of data and squares,
$y_1=z_1=0, y_t=\sum_{i=1}^{t-1} x_i, z_t=\sum_{i=1}^{t-1} x_i^2$, for
all $t\in\{2,\dots,n+1\}$.

Assume there is some set $\mathcal S$ of segments that could be split,
each $(j,e)\in\mathcal S$ is a segment start $j$ and end
$e$ (both in $1,\dots,n$).

Then the next segment to split $(j,e)$, and best split point $t$, are defined by
the best loss decrease,

$$
\max_{(j,e)\in\mathcal S}
\alert{\underbrace{L_{j,e}}_{\text{loss before split}}}-
\min_{t\in\{j,\dots,e-1\}} 
\underbrace{L_{j,t} + L_{t+1,e}}_{\text{loss after split}} 
$$

Use the cumsum trick to compute loss in constant time,
$$\sum_{i=j}^t x_i = y_{t+1} - y_j.$$
$$L_{j,t} = z_{t+1}-z_j - (y_{t+1}-y_j)^2/(t-j+1).$$

## Learning algorithm, recursion/pseudo-code

Notation. Let $D_{j,e}(t)=L_{j,t}+L_{t+1,e}-L_{j,e}$ be the loss difference after splitting segment $(j,e)$ at $t$, and
let $f(j,e)=\min,\argmin_{t\in \{j,\dots,e-1\}} D_{j,e}(t)$ be the best loss difference/split on segment $(j,e)$.

Initialization. Let $\mathcal L_1=L_{1,n}$ be the loss with one segment, 
and let $\mathcal S_1=\{(1,n,f(1,n))\}$ be the initial segment to split.

Recursion. For all $k\in\{2,\dots,K\}$: (max segments $K\leq n$)

* $j^*_k, e^*_k, d^*_k, t^*_k = \argmin_{(j,e,d,t)\in\mathcal S_{k-1}} d$ (best segment to split)
* $\mathcal L_k=\mathcal L_{k-1} +d^*_k$, (loss)
* $\mathcal N_k=\{
(j^*_k, t^*_k),
(t^*_k+1,e^*_k)
\}$ (new segments)
* $\mathcal V_k=\{(j,e,f(j,e))\mid (j,e)\in\mathcal N_k,\, j<e\}$ (splittable segments)
* $\mathcal S_k=
[\mathcal S_{k-1}\setminus (j^*_k, e^*_k, d^*_k, t^*_k)]
\cup\mathcal V_k$ (segments to search)

## Modification for min segment length

Sometimes there is prior knowledge that there should be no segments
with fewer data points than $\ell\in\{1,2,\dots\}$, and in that case
there is a simple modification:

* $f(j,e)=\min,\argmin_{t\in\{
\alert{j+\ell-1,\dots,e-\ell}
\}} D_{j,e}(t)$ (best split)
* $\mathcal V_k=\{(j,e,f(j,e))\mid (j,e)\in\mathcal N_k,\, 
\alert{e-j+1\geq 2\ell}\}
\}$ (splittable segments)

# Finite Sample Complexity Analysis of Binary Segmentation (arXiv:2410.08654)

## Visualization of computations at each iteration

```{r}
one.pid.chr <- nb.dt[profile.id==2 & chromosome==2]
one.pid.chr[, data.i := .I]
cum.data.vec <- c(0, cumsum(one.pid.chr[["logratio"]]))
possible_splits <- function(seg.dt){
  some.segs <- seg.dt[full_seg_start<full_seg_end]
  if(nrow(some.segs)==0)return(NULL)
  possible.dt <- some.segs[, {
    before_seg_end <- seq(full_seg_start, full_seg_end-1)
    data.table(
      before_seg_start=full_seg_start,
      before_seg_end,
      after_seg_start=before_seg_end+1L,
      after_seg_end=full_seg_end
    )
  }, by=.(full_seg_start, full_seg_end)]
  name <- function(suffix)paste0(seg_name, "_seg_", suffix)
  value <- function(suffix)possible.dt[[name(suffix)]]
  for(seg_name in c("before", "after", "full")){
    end <- value("end")
    start <- value("start")
    N.data <- end-start+1
    sum.data <- cum.data.vec[end+1]-cum.data.vec[start]
    set(
      possible.dt,
      j=name("loss"),
      value=-sum.data^2/N.data)
    set(
      possible.dt,
      j=name("mean"),
      value=sum.data/N.data)
  }
  possible.dt[
  , split_loss := before_seg_loss + after_seg_loss][
  , loss_diff := split_loss-full_seg_loss][]
}
get_segs <- function(best){
  nc::capture_melt_multiple(
    best,
    seg="before|after",
    "_seg_",
    column="start|end|mean"
  )[order(start)][, startChange := c(FALSE, TRUE)]
}
get_vlines <- function(segs){
  segs[start>1][startChange==FALSE, computed := "previously"]
}
(prev.best.tall <- data.table(start=1L, end=nrow(one.pid.chr)))
prev.best.i <- NULL
cum.cand <- 0
plot.iteration <- function(){
  (it.segs <- prev.best.tall[, .(
    full_seg_start=start, full_seg_end=end)])
  (it.possible <- possible_splits(it.segs))
  prev.not.best <- if(!is.null(prev.best.i))prev.splits.dt[-prev.best.i]
  this.cand <- if(is.null(it.possible)) 0 else nrow(it.possible)
  cum.cand <<- cum.cand+this.cand
  cand.this.step <- data.table(this.cand, cum.cand)
  it.possible.show <- rbind(
    if(!is.null(prev.best.i))data.table(
      computed="previously",
      prev.not.best),
    if(!is.null(it.possible))data.table(
      computed="this step", it.possible))
  (it.splits.dt <- rbind(
    if(!is.null(prev.best.i))prev.not.best,
    if(!is.null(it.possible))it.possible[, {
      .SD[which.min(loss_diff)]
    }, by=.(full_seg_start, full_seg_end)]))
  it.best.i <- it.splits.dt[, which.min(loss_diff)]
  it.best <- it.splits.dt[it.best.i]
  it.best.tall <- get_segs(it.best)
  it.segs.dt <- rbind(
    if(!is.null(prev.best.i))prev.segs.dt[
      !it.best, on=c(start="full_seg_start", end="full_seg_end")
    ][, computed := "previously"],
    data.table(computed="this step", it.best.tall))
  it.vlines <- get_vlines(it.segs.dt)
  computed.colors <- c(
      "this step"="red",
      "previously"="deepskyblue")
  gg <- ggplot()+
    facet_grid(panel ~ ., scales="free")+
    geom_blank(aes(
      0,0,color=computed),
      data=data.table(computed=names(computed.colors)))+
    scale_color_manual(
      values=computed.colors,
      breaks=names(computed.colors))+
    scale_x_continuous(
      "data.i (position/index in data sequence)",
      limits=c(0, nrow(one.pid.chr)+1))+
    scale_y_continuous(
      "logratio (noisy copy number measurement)",
      labels=scales::scientific)+
    geom_point(aes(
      data.i, logratio),
      data=data.table(panel="data", one.pid.chr))+
    geom_point(aes(
      before_seg_end+0.5, loss_diff, color=computed),
      shape=1,
      data=data.table(panel="loss difference", it.possible.show))+
    geom_vline(aes(
      xintercept=start-0.5,
      color=computed),
      data=data.table(
        panel="data",
        it.vlines[computed=="previously"]))+
    geom_vline(aes(
      xintercept=start-0.5,
      color=computed),
      data=it.vlines[computed=="this step"])+
    geom_segment(aes(
      start-0.5, mean,
      color=computed,
      xend=end+0.5, yend=mean),
      linewidth=2,
      alpha=0.5,
      data=data.table(panel="data", it.segs.dt))+
    geom_label(aes(
      Inf, Inf,
      label = sprintf("candidates this step= %d, cumulative= %d", this.cand, cum.cand),
      color=computed),
      alpha=0.75,
      hjust=1,
      vjust=1,
      data=data.table(panel="loss difference", computed="this step", cand.this.step))
  suppressWarnings(print(gg))
  prev.best.i <<- it.best.i
  prev.best.tall <<- it.best.tall
  prev.segs.dt <<- it.segs.dt
  prev.splits.dt <<- it.splits.dt
}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Visualization of computations at each iteration

```{r}
plot.iteration()
```

## Complexity analysis

- Assume $n$ data and $K$ segments.
- Computing best loss decrease and split point for a segment with $t$
  data takes $O(t)$ time (for square loss).
- Keep a list of segments which could be split, sorted by loss
  decrease values.
- Best case is when segments get cut in half each time, $O(n \log K)$
  time. (minimize number of possible splits for which we have to recompute loss)
- Worst case is when splits are very unequal (1, $t-1$), $O(n K)$
  time. (maximize number of possible splits for which we have to
  recompute loss)

## Detailed complexity analysis

- Let $n=2^J$ for some $J\in\{1,2,\dots\}$, for example $J=6
  \Rightarrow n=64$.
- For any $j\in\{1,\dots,J+1\}$ if we do $K=2^{j-1}$ iterations
  then how many split cost values to compute?
- Best case: $nj -2^j + 1 = n(1+\log_2 K) -K/2 +1 \Rightarrow O(n \log K)$.
- Worst case: $nK - K(1+K)/2 \Rightarrow O(nK)$.
 
```{=latex}
\small
\begin{tabular}{cccccc}
$j$ & $K$ & best & total & worst & total \\
\hline
1 & 1 & $n-1=63$  & $n-1=63$    & $n-1=63$ & $n-1=63$ \\
2 & 2 & $n-2=62$  & $2n-3=125$  & $n-2=62$ & $2n-3=125$ \\
  & 3 & $n/2-2=30$&             & $n-3=61$ & $3n-6=186$ \\
3 & 4 & $n/2-2=30$& $3n-7=185$  & $n-4=60$ & $4n-10=246$ \\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$\\
4 & 8 & $n/4-2=14$& $4n-15=241$ & $n-8=56$ & $8n-36=476$ \\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ \\
7 & 64 & $n/32-2=0$&$7n-128=321$ & $n-64=0$ & $64n-2080=2016$ \\
\end{tabular}
```

## Real data time complexity analysis

```{r, fig.height=4}
N.exp <- 6
N.data <- 2^N.exp
N.exp.seq <- 1:N.exp
(size.after.split <- as.integer(N.data/rep(2^N.exp.seq, 2^(N.exp.seq-1))))
new.splits <- data.table(
  best=(size.after.split-1)*2,
  worst=seq(N.data-2, 0))
split.dt.list <- list()
for(case in names(new.splits)){
  split.dt.list[[case]] <- data.table(
    case,
    segments=seq(1, N.data),
    splits=c(N.data-1, new.splits[[case]]))
}
(split.dt <- do.call(rbind, split.dt.list))
(splits.wide <- dcast(split.dt, segments ~ case, value.var="splits"))
approx.dt <- one.dt[, {
  pos <- seq(min(position), max(position), l=N.data)
  data.table(
    pos,
    data.i=seq_along(pos),
    logratio=approx(position, logratio, pos)[["y"]])
}]
data.list <- list(
  linear=1:N.data,#best
  sin=sin((1:N.data)/2),
  constant.noiseless=rep(0, N.data),
  real=approx.dt[["logratio"]],
  constant.noisy=rnorm(N.data),
  exponential=2^(1:N.data),
  up.and.down=(1:N.data) %% 2)#worst
fit.dt.list <- list()
data.dt.list <- list()
segs.dt.list <- list()
for(data.name in names(data.list)){
  bad.data.vec <- data.list[[data.name]]
  print(data.name)
  fit <- suppressWarnings(binsegRcpp::binseg_normal(bad.data.vec))
  s <- function(x)ifelse(x==1, 0, 1)
  new.segs <- fit$splits[, s(after.size)+s(before.size)]
  map.size.after.erase <- cumsum(c(1, new.segs[-1]-1))
  ### print(data.name)
  ### print(map.size.after.erase)
  coef.dt <- coef(fit, 2:5)
  coef.dt[, panel := segments]
  segs.dt.list[[data.name]] <- data.table(
    data.name,
    coef.dt)
  data.dt.list[[data.name]] <- data.table(
    data.name,
    data.i=seq_along(bad.data.vec),
    data.value=bad.data.vec
    )
  ##set(fit, j="data", value=bad.data.vec)#segfault TODO post issue.
  ##fit[["data"]] <- bad.data.vec
  fit.dt.list[[data.name]] <- data.table(
    data.name,
    data=bad.data.vec,
    fit$splits)
}

(fit.dt <- do.call(rbind, fit.dt.list))
(data.dt <- do.call(rbind, data.dt.list))
(segs.dt <- do.call(rbind, segs.dt.list))
fit.dt[, splits := before.size-1+ifelse(is.na(after.size), 0, after.size-1)]
showCase <- function(show.name){
  show.data <- data.dt[data.name==show.name]
  show.segs <- segs.dt[data.name==show.name]
  show.fit <- fit.dt[data.name==show.name]
  gg.data <- ggplot()+
    geom_point(aes(
      data.i, data.value),
      data=show.data)+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      data=show.segs,
      linewidth=2,
      alpha=0.5,
      color=model.color)+
    geom_vline(aes(
      xintercept=start-0.5),
      data=show.segs[start>1],
      color=model.color)+
    facet_grid(segments ~ ., labeller=label_both)
  print(gg.data)
  emp.case <- paste0("empirical\n", show.name)
  emp.case <- "empirical"
  show.splits <- rbind(
    split.dt,
    show.fit[, .(case=emp.case, segments, splits)])
  case.colors <- c("deepskyblue", "black", "red")
  names(case.colors) <- c("worst", emp.case, "best")
  show.totals <- show.splits[, .(total.splits=sum(splits)), by=case][
    names(case.colors), on="case"]
  show.totals[, label := sprintf("case=%s\nsplits=%d", case, total.splits)]
  show.dl <- show.totals[show.splits, on="case"]
  show.totals[, y := N.data-.I*5]
  gg.splits <- ggplot()+
    ggtitle(paste0(
      "Number of splits for which loss is computed, empirical data type=",
      show.name))+
    geom_line(aes(
      segments, splits, color=case),
      data=show.splits[!grepl("empirical", case)])+
    geom_point(aes(
      segments, splits, color=case),
      data=show.splits[grepl("empirical", case)])+
    geom_text(aes(
      N.data, y, label=sprintf(
        "%s case total splits=%d",
        case, total.splits),
      color=case),
      hjust=1,
      data=show.totals)+
    scale_color_manual(
      values=case.colors,
      breaks=names(case.colors),
      guide="none")
  print(gg.splits)
}
max_segs <- function(max.seg.num){
  N.data <- 8L
  min.seg.len <- 1L
  max.segments <- as.integer(max.seg.num)
  cost.dt <- binsegRcpp::get_complexity_best_optimal_cost(
    N.data, min.seg.len, max.segments)
  set.seed(1)
  best.data <- if(max.segments<=4){
    c(seq(1,max.segments-1), rep(N.data,N.data-max.segments+1))
  }else{
    c(1:4, (4:1 -1)*4)
  }
  data.list <- list(
    Worst=rep(1:2,l=N.data),
    Best=best.data,
    Equal=1:N.data)
  library(data.table)
  (data.dt <- data.table(splits=names(data.list))[, data.table(
    y="data",
    value=as.numeric(data.list[[splits]]),
    position=1:N.data
  ), by=splits])
  fit.list <- suppressWarnings(lapply(data.list, binsegRcpp::binseg_normal, max.segments))
  ord.dt <- data.table(splits=names(data.list))[
  , fit.list[[splits]]$splits
  , by=splits][, y := "data"]
  coef.dt <- data.table(splits=names(data.list))[
  , coef(fit.list[[splits]], max.segments)
  , by=splits][, y := "data"]
  tree.dt <- data.table(splits=names(data.list))[, {
    fit <- fit.list[[splits]]
    tree.dt <- binsegRcpp::get_tree_empirical(fit)
    binsegRcpp::tree_layout(tree.dt)
  }, by=splits][, y := "depth"]
  total.dt <- tree.dt[, .(
    candidates=sum(binsegRcpp::size_to_splits(size, min.seg.len))
  ), by=splits][, let(
    Splits=paste(splits, "splits"),
    Candidates=paste(candidates, "candidates")
  )]
  join.tree <- total.dt[tree.dt, on="splits"]
  join.data <- total.dt[data.dt, on="splits"]
  join.coef <- total.dt[coef.dt, on="splits"]
  join.ord <- total.dt[ord.dt, on="splits"]
  x.off <- (N.data+1)/2
  ggplot()+
    ggtitle(sprintf("N_data=%d, Max segments=%d", N.data, max.segments))+
    theme_bw()+
    facet_grid(y ~ Splits + Candidates, scales="free")+
    geom_point(aes(
      position, value),
      data=join.data)+
    geom_vline(aes(
      xintercept=start.pos),
      linetype="dashed",
      color="green",
      data=join.coef[start>1])+
    geom_label(aes(
      end+0.5, Inf,
      label=segments-1),
      color="green",
      vjust=1,
      data=join.ord[segments>1])+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      linewidth=1,
      color="green",
      data=join.coef)+
    geom_segment(aes(
      x+x.off, depth,
      xend=parent.x+x.off, yend=parent.depth),
      data=join.tree[!is.na(parent)])+
    geom_label(aes(
      x+x.off, depth,
      label=size),
      ##label=sprintf("n=%d\nc=%d",size,size-1)),
      data=join.tree)+
    scale_y_continuous("", breaks=seq(0,100))+
    scale_x_continuous("", breaks=NULL)+
    theme(panel.grid.minor=element_blank())
}

showCase("real")
```

## Synthetic data time complexity analysis

```{r, fig.height=4}
showCase("up.and.down")
```

## Synthetic data time complexity analysis

```{r, fig.height=4}
showCase("exponential")
```

## Synthetic data time complexity analysis

```{r, fig.height=4}
showCase("sin")
```

## Synthetic data time complexity analysis

```{r, fig.height=4}
showCase("linear")
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(2)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(3)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(4)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(5)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(6)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(7)
```

## Synthetic data time complexity, vary max segments

```{r}
max_segs(8)
```

## Optimal binary trees

\includegraphics{figure-optimal-trees-some}

- $n\in\{60,71,72,80\}$ data, min segment length $m=5$, and number of splits/iterations $K=9$.
- Smaller $n\in\{60,71\}$ values result in a balanced first split,
- Larger $n\in\{72,80\}$ values result in an unbalanced first split (one small child with no splits, one large child with all remaining splits).
- [Hocking arXiv:2410.08654 Finite Sample Complexity Analysis of Binary Segmentation](https://arxiv.org/abs/2410.08654) propose $O(n^2 K^2)$ algorithm to compute optimal binary tree.
- Is binary segmentation asymptotically optimal speed in real data?

## Empirical number of candidates (Poisson loss)

\includegraphics[width=\textwidth]{figure-mcgill-iterations.png}

- 2752 real genomic count data sets from McGill benchmark of data size $n$ from 87 to 263169, using binary segmentation with the Poisson loss. 
- Number of candidate splits to consider in real data (grey dots) achieves the asymptotic best case, $O(n\log n)$.

## Empirical number of candidates (Square loss)

\includegraphics[width=\textwidth]{figure-neuroblastoma-iterations.png}

- 13721 real genomic data sets from neuroblastoma benchmark of data size $n$ from 11 to 5937, using binary segmentation with the square loss. 
- Number of candidate splits achieves the asymptotic best case, $O(n\log n)$, and in 45 instances (black circles) requires fewer candidate splits than predicted by the best case heuristic.
- For max segments = 10, we used dynamic programming to compute the best case number of splits for all data sizes between 11 and 100 (orange line), and the heuristic (green line) was exact for only 5 data sizes $n\in\{11,12,13,18,19\}$ (purple circles).

# Comparing binsegRcpp with other implementations of binary segmentation (under review at Journal of Statistical Software)

## Two key steps in binary segmentation algorithm

- Compute best split on a segment (red).
- Find best segment to split (blue).

\includegraphics[width=\textwidth]{05-this-previously.png}

## Analysis of insert time and storage (STL list)

To store previously computed best loss/split for each segment, use
C++ Standard template library list as baseline.

- If list has $p$ items then insert takes $O(1)$ time.
- Find best segment to split: $O(p)$ time.
- Total $O(K^2)$ time for $K$ iterations (equal splits), larger than $O(n\log K)$ time for computing candidate loss values.

```{=latex}
\small
\begin{tabular}{ccc}
splits $K$     & items in list (equal splits)   &   items in list (unequal splits)   \\
\hline
 1   & 1  & 1 \\
 2 & 2 &  1 \\ 
 3 & 3 &  1 \\
$\vdots$  &     $\vdots$  & $\vdots$ \\
32 & 32  & 1 \\
33       & 31 &  1 \\
34       & 30 &  1 \\
$\vdots$  &     $\vdots$  & $\vdots$ \\
62        & 2 &  1 \\
63        & 1  & 1
\end{tabular}
```

## Analysis of insert time and storage (STL multiset)

To achieve asymptotic best case timings, use a
C++ Standard template library multiset, keyed by loss decrease. If
multiset has $p$ items then insert takes $O(\log p)$ time. Below:
inserts column shows $p$ for each insert, and size column shows $p$
after inserts. Total
$-\log(K-1) + \sum_{p=1}^{K-1} 2\log p\in O(K\log K)$
time over all inserts, smaller than $O(n\log K)$ time for split computation.

```{=latex}
\small
\begin{tabular}{ccccc}
     & equal splits   &      & unequal splits &  \\
 iteration $K$ & inserts & size & inserts & size \\
\hline
 1 & 0   & 1 & 0 & 1 \\
 2 & 0,1 & 2 & 0 & 1 \\ 
 3 & 1,2 & 3 & 0 & 1 \\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ & $\vdots$ \\
32 & 30,31 & 32 & 0 & 1 \\
33 &       & 31 & 0 & 1 \\
34 &       & 30 & 0 & 1 \\
$\vdots$ &     $\vdots$ &     $\vdots$  & $\vdots$ & $\vdots$ \\
62 &       & 2 & 0 & 1 \\
63 &       & 1 & 0 & 1
\end{tabular}
```

## Timings (black) with asymptotic references (violet)

\includegraphics[width=\textwidth]{figure-heap-refs-binsegRcpp-tex.png}

- Number of segments grows with data size, $K=O(N)$.
- Best case data, square loss.
- Linear space, $O(N)$.
- binsegRcpp multiset is $O(N\log N)$ time, asymptotically
  faster than binsegRcpp list, $O(N^2)$ time.

## Timings (black) with asymptotic references (violet)

\includegraphics[width=\textwidth]{figure-heap-refs-others-tex.png}

- Number of segments grows with data size, $K=O(N)$.
- Best case data, square loss.
- change-point is $O(N^2)$ space, $O(N^3)$ time.
- ruptures is $O(N)$ space, $O(N^{1.4})$ time (unclear why)
- Not asymptotic optimal complexity.
- Sub-optimal implementation.

## Timing and throughput comparisons

\includegraphics[width=\textwidth]{figure-heap-pred.png}

- Number of segments grows with data size, $K=O(N)$.
- Best case data, square loss.
- in 5 seconds, binsegRcpp using multiset or priority queue has 100x larger
  throughput than list.
- about 500x larger throughput than ruptures.
- about 1000x larger throughput than change-point.

## Comparing implementations of binary segmentation

\includegraphics{table-compare-binseg-packages}

Hocking 2025 Comparing binsegRcpp to other implementations of binary
segmentation, in progress.

## Timings using square loss

\includegraphics[width=\textwidth]{figure-timings.png}

- binsegRcpp map, `wbs::sbs`, `fpop::multiBinSeg` optimal: $O(n\log n)$ time in best case.
- ruptures is between $O(n\log n)$ and $O(n^2)$ time, but with large constant factors.
- binsegRcpp list has larger slope: $O(n^2)$ time.
- change-point has even larger slope: $O(n^3)$ time.

## Timings using L1 loss

\includegraphics[width=\textwidth]{figure-timings-laplace.png}

- Number of segments grows with data size, $K=O(n)$.
- Best case: binsegRcpp multiset is $O(n\log n)$, asymptotically
  faster than binsegRcpp list, $O(n^2)$.
- ruptures is significantly slower (100x).
- Worst case: all algorithms are quadratic, because that is the number
  of candidate change-points that must be considered.

## Timings using Gaussian change in mean and variance model

\includegraphics[width=\textwidth]{figure-timings-meanvar_norm.png}

## Timings using Poisson loss

\includegraphics[width=\textwidth]{figure-timings-poisson.png}

## Comparing loss functions, binsegRcpp multiset

\includegraphics[width=\textwidth]{figure-compare-distributions.png}

- Laplace (log-linear) is somewhat slower than others (linear).

## Accuracy of library(binsegRcpp) and library(changepoint)

\includegraphics[width=\textwidth]{changepoint.bug.png}

- Real data set for which change-point does not compute the correct model.
- Loss should be minimized at each iteration.

## Conclusions

- Binary segmentation is an important algorithm for change-point detection in sequential data.
- Optimal binary trees used to determine best case number of
  candidates that the algorithm needs to consider.
- Empirical number of candidates in real data: same as best case, asymptotically.
- Proposed binsegRcpp R package provides C++ code which is asymptotically optimal speed, and correct.
- `install.packages("binsegRcpp")` in R.
- Future work: faster method for computing optimal binary trees? $O(n^2 K^2)$ time for lower bound is slower than $O(n K)$ time for algorithm itself.

## Comparison with previous algorithms from clustering

- Binary segmentation has segment/cluster-specific mean parameter, as
  in K-means and Gaussian mixture models. These algorithms attempt
  optimization of an error function which measures how well the means
  fit the data (but are not guaranteed to compute the globally
  optimal/best model).
- Binary segmentation is deterministic (different from K-means/GMM
  which requires random initialization). It performs a sequence of
  greedy minimizations (as in hierarchical clustering).
- Binary segmentation defines a sequence of split operations (from 1
  segment to N segments), whereas agglomerative hierarchical
  clustering defines a sequence of join operations (from N clusters to
  1 cluster). Data with common segment mean must be adjacent in
  time/space; hierarchical clustering joins may happen between any
  pair of data points (no space/time dimension).

## Possible exam questions

- Explain in detail one similarity and one difference between binary
  segmentation and k-means. (gaussian mixture models, hierarchical
  clustering)
- For a sequence of $n=10$ data, we need to compute the loss for each
  of the 9 possible splits in the first iteration of binary
  segmentation. What is the number of splits for which we must compute
  the loss in the second/third steps? (best and worst case)
