The goal of this homework is to use an auto-encoder to learn a low
dimensional nonlinear mapping of a high dimensional data set, and
compare to the PCA linear mapping.

See [[file:13-auto-encoders-torch.R][torch]] coding demo.

1. first select 100 rows of the zip.train data from the ESL book (10
   from each class). Or you could use another digit data like the
   MNIST I used in my slides, but MNIST is larger so the zip data
   should be easier/faster. For these data use the torch R package to
   define an auto-encoder, using torch::nn_sequential,
   torch::nn_linear, and torch::nn_module. The module initialize method should
   define self$encoder and self$decoder. Use torch::nn_relu activation
   after intermediate linear layers (and no activation after the last
   layer).  For visualization purposes make sure the code layer has
   two units. How many parameters are there to learn in this nonlinear
   model? How many parameters are there to learn in the corresponding
   PCA linear model with rank=2?  (this is the number of entries in
   the first two columns of the rotation matrix, plus the number of
   entries in the mean vector) Is the number of parameters in the
   auto-encoder larger as expected?

2. Now learn the auto-encoder parameters using luz::setup (with
   loss=torch::nnf_mse_loss and optimizer=torch::optim_*) and
   luz::fit. Use the predict function to compute the predicted
   values. Also compute a PCA with rank=2 and compute its predicted
   values. What is the reconstruction error (mean squared error
   between data and predicted values) for the two methods? Is the
   auto-encoder more accurate as expected? (for full credit make sure
   that it is) If not, try some of the following
   - increasing the number epochs, and/or the learning rate.
   - use a different optim_* function. (I got it to work with adadelta in keras in a previous semester)
   - make sure the last activation is linear, and all other activations are non-linear (relu or sigmoid).

3. Now use fitted$model$encode(features) to compute the
   low-dimensional embedding of the original train data (where fitted
   is the output of luz::fit). Make a ggplot with these auto-encoder
   embeddings in one panel, and the PCA in another panel, using
   facet_wrap(scales="free") so that the plots are NOT constrained to
   have the same scales on the x/y axes (the units of the PCA and
   auto-encoder embeddings are not comparable). Use
   geom_text(label=digit) or geom_point(color=digit) to visualize the
   different digit classes. Which of the two methods results in better
   separation between digit classes?

** FAQ

How to fix compiler not found on windows? 

Try to install https://cloud.r-project.org/bin/windows/Rtools/ which is
a set of compilers which allows you to build and install R packages
from source. 

** CS599 graduate students only

Your job is to investigate how the auto-encoder model architecture
affects overfitting.
- First decide on two different auto-encoder architectures of varying
  complexity that you would like to compare. For example you may
  compare a (256,100,10,2,10,100,256) to (256,10,2,10,256) to see if
  adding layers affects overfitting. Or you could compare
  (256,10,2,10,256) to (256,100,2,100,256) to see if the number of
  intermediate units affects overfitting.
- Create a variable named model.list, which should be a list of the
  two keras models described above. Make a for loop over these two
  models, and use a 50% subtrain, 50% validation split.
- Make a ggplot of y=square loss as a function of x=iterations, with
  different sets in different colors (e.g., subtrain=black,
  validation=red), and the two different models in two different
  panels, facet_grid(. ~ model). Does either model overfit?
- Finally make another ggplot which displays the low dimensional
  embeddings, as in problem 3 above. Which of the two methods results
  in better separation between digit classes?
